{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK ON NATIVE GRIDS!!! This will allow access to more models\n",
    "No - how do you define teh S. Ocean on a crazy grid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import matplotlib.pyplot as plt\n",
    "import iris.quickplot as qplt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import iris.coord_categorisation\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle\n",
    "import iris.coord_categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearsec=60.0*60.0*24.0*365\n",
    "kg2mol_C = 1000.0/12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_avg_with_areacello(cube,cube_areacello):\n",
    "    cube_areacello_tmp = cube_areacello.copy()\n",
    "    no_time_steps = len(cube.coord('time').points)\n",
    "    tmp = cube.copy()\n",
    "    if no_time_steps > 1:\n",
    "        tmp.data = cube.data * np.tile(cube_areacello_tmp.data,[no_time_steps,1,1])\n",
    "    else:\n",
    "        tmp.data = cube.data * cube_areacello_tmp.data\n",
    "    cube_areacello_masked = cube_areacello_tmp.copy()\n",
    "    cube_areacello_masked.data = np.ma.masked_array(cube_areacello_masked.data)\n",
    "    if no_time_steps > 1:\n",
    "        cube_areacello_masked.data.mask = cube[0].data.mask\n",
    "    else:\n",
    "        cube_areacello_masked.data.mask = cube.data.mask\n",
    "    tmp /= np.ma.mean(cube_areacello_masked.data)\n",
    "    return tmp.collapsed(['latitude','longitude'],iris.analysis.MEAN)\n",
    "\n",
    "\n",
    "def area_sum_with_areacello(cube,cube_areacello):\n",
    "    cube_areacello_tmp = cube_areacello.copy()\n",
    "    no_time_steps = len(cube.coord('time').points)\n",
    "    tmp = cube.copy()\n",
    "    if no_time_steps > 1:\n",
    "        tmp.data = cube.data * np.tile(cube_areacello_tmp.data,[no_time_steps,1,1])\n",
    "    else:\n",
    "        tmp.data = cube.data * cube_areacello_tmp.data\n",
    "    cube_areacello_masked = cube_areacello_tmp.copy()\n",
    "    cube_areacello_masked.data = np.ma.masked_array(cube_areacello_masked.data)\n",
    "    if no_time_steps > 1:\n",
    "        cube_areacello_masked.data.mask = cube[0].data.mask\n",
    "    else:\n",
    "        cube_areacello_masked.data.mask = cube.data.mask\n",
    "    return tmp.collapsed(['latitude','longitude'],iris.analysis.SUM)\n",
    "\n",
    "\n",
    "def model_names(directory):\n",
    "\tfiles = glob.glob(directory+'/*.nc')\n",
    "\tmodels_tmp = []\n",
    "\tfor file in files:\n",
    "\t\tstatinfo = os.stat(file)\n",
    "\t\tif statinfo.st_size >= 1:\n",
    "\t\t\tmodels_tmp.append(file.split('/')[-1].split('_')[0])\n",
    "\t\t\tmodels = np.unique(models_tmp)\n",
    "\treturn models\n",
    "\n",
    "def model_names_var(directory,var,grid):\n",
    "\tfiles = glob.glob(directory+'/*'+var+'*'+grid+'*.nc')\n",
    "\tmodels_tmp = []\n",
    "\tfor file in files:\n",
    "\t\tstatinfo = os.stat(file)\n",
    "\t\tif statinfo.st_size >= 1:\n",
    "\t\t\tmodels_tmp.append(file.split('/')[-1].split('_')[0])\n",
    "\t\t\tmodels = np.unique(models_tmp)\n",
    "\treturn models\n",
    "\n",
    "def model_names_var_cmip5(directory,var):\n",
    "\tfiles = glob.glob(directory+'/*'+var+'*.nc')\n",
    "\tmodels_tmp = []\n",
    "\tfor file in files:\n",
    "\t\tstatinfo = os.stat(file)\n",
    "\t\tif statinfo.st_size >= 1:\n",
    "\t\t\tmodels_tmp.append(file.split('/')[-1].split('_')[0])\n",
    "\t\t\tmodels = np.unique(models_tmp)\n",
    "\treturn models\n",
    "\n",
    "def mask_where_zero(cube):\n",
    "    cube.data = np.ma.masked_array(cube.data)\n",
    "    cube.data.fill_value= 9.99e9\n",
    "    cube.data[np.where(cube.data == 0.0)] = 9.99e9\n",
    "    cube.data = np.ma.masked_where(cube.data == 9.99e9,cube.data)\n",
    "    return cube\n",
    "\n",
    "def extract_region(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube_region_tmp = cube.intersection(longitude=(lon_west, lon_east))\n",
    "    cube_region = cube_region_tmp.intersection(latitude=(lat_south, lat_north))\n",
    "    return cube_region\n",
    "\n",
    "\n",
    "def area_sum(cube):\n",
    "#     first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "#     second_dim = cube.coord(dimensions=2).long_name #longitude\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    area_avged_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.SUM, weights=grid_areas)\n",
    "    return area_avged_cube\n",
    "\n",
    "def area_avg(cube):\n",
    "#     first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "#     second_dim = cube.coord(dimensions=2).long_name #longitude\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    area_avged_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas)\n",
    "    return area_avged_cube\n",
    "\n",
    "def area_sum2(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube = extract_region(cube,lon_west,lon_east,lat_south,lat_north)\n",
    "#     first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "#     second_dim = cube.coord(dimensions=2).long_name #longitude\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    area_avged_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.SUM, weights=grid_areas)\n",
    "    return area_avged_cube\n",
    "\n",
    "def area_avg2(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube = extract_region(cube,lon_west,lon_east,lat_south,lat_north)\n",
    "#     first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "#     second_dim = cube.coord(dimensions=2).long_name #longitude\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    area_avged_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas)\n",
    "    return area_avged_cube\n",
    "\n",
    "def avg_years(cube,start_yr,end_yr):\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    loc = np.where((cube.coord('year').points >= start_yr) & (cube.coord('year').points <= end_yr))\n",
    "    if len(loc[0]) > 0:\n",
    "        return cube[loc].collapsed('time', iris.analysis.MEAN)\n",
    "    else:\n",
    "        cube=cube.collapsed('time', iris.analysis.MEAN)\n",
    "        cube.data[:] = np.nan\n",
    "        return cube\n",
    "\n",
    "                   \n",
    "def return_years(cube):\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    return cube.coord('year').points\n",
    "\n",
    "\n",
    "def year_mean(cube):\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    return cube.aggregated_by('year', iris.analysis.MEAN)\n",
    "\n",
    "def populate_dict(data_dict,directory,models,variable,run,test_value):\n",
    "    lon_west,lon_east,lat_south,lat_north=-80.0,10,0.0,80.0\n",
    "    for model in models:\n",
    "        go_ahead = True\n",
    "        print model\n",
    "        exists = os.path.isfile(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "        if exists:\n",
    "            cube = iris.load_cube(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "#             cube = cube.collapsed(['depth'],iris.analysis.MEAN)\n",
    "            iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "            cube = cube.aggregated_by('year', iris.analysis.MEAN)\n",
    "            test = cube.collapsed(['latitude','longitude','time'],iris.analysis.MEAN)\n",
    "            if test_value:\n",
    "                if test.data > 0.6e-9:\n",
    "                    go_ahead=False\n",
    "            if go_ahead:\n",
    "                first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "                if first_dim == 'latitude':\n",
    "    #                 cube.coord(dimensions=1).rename('latitude')\n",
    "    #                 cube.coord(dimensions=2).rename('longitude')\n",
    "                    cube = extract_region(cube,lon_west,lon_east,lat_south,lat_north)\n",
    "                    data_dict[run][variable][model] = {}\n",
    "                    data_dict[run][variable][model]['timeseries'] = area_sum(cube)\n",
    "                    data_dict[run][variable][model]['timeseries_avg'] = area_avg(cube)\n",
    "                    data_dict[run][variable][model]['years'] = return_years(cube)\n",
    "                    data_dict[run][variable][model]['first20'] = first_20_avg = avg_years(cube,2006,2026)\n",
    "                    data_dict[run][variable][model]['last20'] = last_20_avg = avg_years(cube,2079,2099)\n",
    "        else:\n",
    "            print directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc does not exist'\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def populate_dict_region(data_dict_region,directory,models,variables,runs,test_value,region_bounds):\n",
    "    regions = list(region_bounds)\n",
    "    #     lon_west,lon_east,lat_south,lat_north = W,E,S,N\n",
    "    for i,region in enumerate(regions):\n",
    "        W,E,S,N = region_bounds[region]['W'],region_bounds[region]['E'],region_bounds[region]['S'],region_bounds[region]['N']\n",
    "        for run in runs:\n",
    "            for variable in variables:\n",
    "                for model in models:\n",
    "                    exists = os.path.isfile(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "                    if exists:\n",
    "                        cube = iris.load_cube(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "                        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "                        cube = cube.aggregated_by('year', iris.analysis.MEAN)\n",
    "                        cube = extract_region(cube,W,E,S,N)\n",
    "                        data_dict_region[region][run][variable][model] = {}\n",
    "                        data_dict_region[region][run][variable][model]['timeseries'] = area_sum(cube)\n",
    "                        data_dict_region[region][run][variable][model]['timeseries_avg'] = area_avg(cube)\n",
    "                        data_dict_region[region][run][variable][model]['years'] = return_years(cube)\n",
    "                        data_dict_region[region][run][variable][model]['first20'] = avg_years(cube,2006,2026)\n",
    "                        data_dict_region[region][run][variable][model]['last20'] = avg_years(cube,2079,2099)\n",
    "                    else:\n",
    "                        print directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc does not exist'\n",
    "    return data_dict_region\n",
    "\n",
    "\n",
    "def populate_dict_region_profile(data_dict_region_profiles,directory,models,variables,runs,test_value,region_bounds):\n",
    "    regions = list(region_bounds)\n",
    "    #     lon_west,lon_east,lat_south,lat_north = W,E,S,N\n",
    "    for i,region in enumerate(regions):\n",
    "        W,E,S,N = region_bounds[region]['W'],region_bounds[region]['E'],region_bounds[region]['S'],region_bounds[region]['N']\n",
    "        data_dict_region_profiles[region]={}\n",
    "        for run in runs:\n",
    "            data_dict_region_profiles[region][run]={}\n",
    "            for variable in variables:\n",
    "                data_dict_region_profiles[region][run][variable]={}\n",
    "                for model in models:\n",
    "                    data_dict_region_profiles[region][run][variable][model]={}\n",
    "                    print run, variable, model\n",
    "                    exists = os.path.isfile(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "                    if exists:\n",
    "                        print 'ok'\n",
    "                        cube = iris.load_cube(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "                        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "                        cube = cube.aggregated_by('year', iris.analysis.MEAN)\n",
    "                        tmp_cube = area_avg(extract_region(cube,W,E,S,N))\n",
    "                        first20_profile = avg_years(tmp_cube,2006,2026)\n",
    "                        last20_profile = avg_years(tmp_cube,2079,2099)\n",
    "                        depth = first20_profile.coord('depth').points\n",
    "                        data_dict_region_profiles[region][run][variable][model]['first20_profile'] = first20_profile.data\n",
    "                        data_dict_region_profiles[region][run][variable][model]['last20_profile'] = last20_profile.data\n",
    "                        data_dict_region_profiles[region][run][variable][model]['depths'] = depth\n",
    "                    else:\n",
    "                        print directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc does not exist'\n",
    "    return data_dict_region_profiles\n",
    "\n",
    "\n",
    "def mask_cube_if_no_mask(cube):\n",
    "    try:\n",
    "        dummy = cube[0].data.mask\n",
    "    except:\n",
    "        test = np.where(cube.data == np.nan)\n",
    "        if test > 1e3:\n",
    "            tmp = cube.data.copy()\n",
    "            tmp = np.ma.masked_array(tmp)\n",
    "            tmp2 = np.ma.masked_where(tmp.data == np.nan,tmp)\n",
    "            cube.data = tmp2\n",
    "        test = np.where(cube.data == 0.0)\n",
    "        if test > 1e3:\n",
    "            tmp = cube.data.copy()\n",
    "            tmp = np.ma.masked_array(tmp)\n",
    "            tmp2 = ma.masked_where(tmp.data == 0.0,tmp)\n",
    "            cube.data = tmp2\n",
    "    return cube\n",
    "\n",
    "def collapse_depth(cube):\n",
    "    if len(np.shape(cube)) == 4:\n",
    "        try:\n",
    "            cube = cube.collapsed('depth',iris.analysis.MEAN)\n",
    "        except:\n",
    "            try:\n",
    "                cube = cube.collapsed('lev',iris.analysis.MEAN)\n",
    "            except:\n",
    "                try:\n",
    "                    name = cube.coord(dimensions=1).long_name\n",
    "                    cube = cube.collapsed(name,iris.analysis.MEAN)\n",
    "                except:\n",
    "                    name = cube.coord(dimensions=1).standard_name\n",
    "                    cube = cube.collapsed(name,iris.analysis.MEAN)\n",
    "    return cube\n",
    "\n",
    "def mask_where_seaice(cube,max_seaice):\n",
    "    sic_mask = np.tile(max_seaice.data,[np.shape(cube)[0],1,1])\n",
    "    cube_data = cube.data.copy()\n",
    "    cube_data = np.ma.masked_where(sic_mask > 0.0, cube_data)\n",
    "#     print cube_data.mask.dtype\n",
    "#     cube_data.mask[np.where(sic_mask > 0)] = True\n",
    "    cube.data = cube_data\n",
    "    return cube\n",
    "\n",
    "\n",
    "# def mask_where_seaice(cube,max_seaice):\n",
    "#     sic_mask = np.tile(max_seaice.data,[np.shape(cube)[0],1,1])\n",
    "#     cube_data = cube.data.data.copy()\n",
    "#     cube_data = np.array(cube_data)\n",
    "#     cube_data = np.ma.masked_array(cube_data)\n",
    "# #     cube_data.mask[:] = True\n",
    "#     cube_data.mask[np.where(sic_mask > 0.0)] = True\n",
    "#     cube.data = cube_data\n",
    "#     return cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ops= 0 ;  output is iteration count\n",
      "       1 ;            pCO2\n",
      "       2 ;            pH\n",
      "       3 ;            [H2CO3]\n",
      "       4 ;            [HCO3]\n",
      "       5 ;            [CO3]\n",
      "       6 ;            satn [co3] : calcite\n",
      "       7 ;            saturation state: calcite\n",
      "       8 ;            satn [CO3] : aragonite\n",
      "       9 ;            saturation state: aragonite\n",
      "       10;            Ravelle factor (DIC) calculated from Egleston et al. 2010\n",
      "       11;            Alkalinity buffer factor calculated from Egleston et al. 2010\n",
      "inputs: op_swtch,mdi,T,S,TCO2,TALK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntest-data\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Carbon chemistry calulations\n",
    "Input should be:\n",
    "Temperature in K\n",
    "S in PSU (I think)\n",
    "DIC and ALK in MOL? values about 2.0\n",
    "'''\n",
    "\n",
    "'''\n",
    "NOTE - this is currently designed to work with a single time-interval (i.e. cube without a time dimension)\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import scipy.stats\n",
    "import keyword\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "print'  ops= 0 ;  output is iteration count'\n",
    "print'       1 ;            pCO2'\n",
    "print'       2 ;            pH'\n",
    "print'       3 ;            [H2CO3]'\n",
    "print'       4 ;            [HCO3]'\n",
    "print'       5 ;            [CO3]'\n",
    "print'       6 ;            satn [co3] : calcite'\n",
    "print'       7 ;            saturation state: calcite'\n",
    "print'       8 ;            satn [CO3] : aragonite'\n",
    "print'       9 ;            saturation state: aragonite'\n",
    "print'       10;            Ravelle factor (DIC) calculated from Egleston et al. 2010'\n",
    "print'       11;            Alkalinity buffer factor calculated from Egleston et al. 2010'\n",
    "\n",
    "print 'inputs: op_swtch,mdi,T,S,TCO2,TALK'\n",
    "\n",
    "#Supply these as .data (arrays)\n",
    "#salinity needs to be converted into psu *1000+35\n",
    "#TCO2 and TALK must be in mol/kg /(1026.*1000.)\n",
    "#the ones below here are not needed\n",
    "\n",
    "\n",
    "def pressure_fun(a,b,c,d,e,T):\n",
    "    del_vol = np.ones(T.shape, dtype='f')\n",
    "    del_com = np.ones(T.shape, dtype='f') \n",
    "    pf = np.ones(T.shape, dtype='f')\n",
    "    del_vol = a + b *T + c * np.power(T,2.0)\n",
    "    del_com = 1.0e-3*( d + e*T )\n",
    "    pf = np.exp( ( 0.5*del_com*Pr   - del_vol )*Pr / ( 83.131*TK ) )\n",
    "    return pf\n",
    "\n",
    "def carbiter(T, TCO2, TALK, TB, msk, tol, mxiter, K1, K2, KB, KW):\n",
    "    aH = np.empty_like(T, dtype='f')\n",
    "    aH.fill(1.0e-8)\n",
    "    count = np.zeros_like(T)\n",
    "    tol_swtch = np.zeros_like(T)\n",
    "#MB -\n",
    "#    AB = np.ones(T.shape)\n",
    "#    AC = np.ones(T.shape)\n",
    "#    AW = np.ones(T.shape)\n",
    "    \n",
    "    #MB+\n",
    "    TBKB = TB * KB\n",
    "    K2_K1x4 = 4.0 * K2 / K1\n",
    "    K2_2 = 0.5 * K1\n",
    "    #\n",
    "    \n",
    "    iter = 0\n",
    "    test = 2.0\n",
    "    while test > 0.5 and iter < mxiter:\n",
    "        # Compute alkalinity guesses for Boron, Silicon, Phosphorus and Water\n",
    "        #MB- AB = TB * KB / (aH + KB)\n",
    "        #AB = TBKB / (aH + KB)\n",
    "        AB = np.divide(TBKB,(aH + KB))\n",
    "        #  ASi = TSi*KSi/( aH $\n",
    "        #    + KSi )\n",
    "        #  AP = TP*( 1.0/( 1.0 + KP2/aH $\n",
    "        #    + KP2*KP3/(aH^2.0) ) + 2.0/( 1.0 $\n",
    "        #    + aH/KP2 + KP3/aH ) $\n",
    "        #    + 3.0/( 1.0 + aH/KP3 $\n",
    "        #    + (aH^2.0)/(KP2*KP3) ) )\n",
    "        AW = (KW / aH) - aH\n",
    "        # using the guessed alkalinities and total alkalinity, calculate the\n",
    "        # alkalinity due to carbon\n",
    "        #  AC = TALK - ( AB + ASi $\n",
    "        #    + AP + AW )\n",
    "        AC = TALK - (AB + AW)\n",
    "        # and recalculate aH with the new As\n",
    "        #MB+\n",
    "        TCO2_AC = TCO2 - AC\n",
    "        #\n",
    "        old_aH = np.copy(aH)\n",
    "        #MB- aH = (0.5 * K1 / AC) * ((TCO2 - AC) + np.sqrt((TCO2 - AC) * (TCO2 - AC) + 4.0 * (AC * K2 / K1) * (2.0 * TCO2 - AC)))\n",
    "        temp = TCO2_AC*TCO2_AC + (AC * K2_K1x4) * (2.0 * TCO2 - AC)\n",
    "        aH = (K2_2 / AC) * (TCO2_AC + np.sqrt(temp))\n",
    "        tol_swtch = abs((aH - old_aH) / old_aH) > tol\n",
    "        count = count + tol_swtch\n",
    "        test = np.sum(tol_swtch)\n",
    "        iter += 1\n",
    "        \n",
    "    #aH[~msk] = 1.0\n",
    "    #count[~msk] = 0\n",
    "    return AC, AW, AB, aH, count\n",
    "\n",
    "\n",
    "def carbchem_revelle(op_swtch,mdi,T_cube,S_cube,TCO2_cube,TALK_cube,Pr=0.0,TB=0.0,Ni=100.0,Tl=1.0e-5):\n",
    "# This function calculates the inorganic carbon chemistry balance\n",
    "# according to the method of Peng et al 1987\n",
    "# The parameters are set in the first few lines\n",
    "\n",
    "#salinity needs to be converted into psu\n",
    "#TCO2 and TALK must be in mol/kg\n",
    "#the ones below here are not needed\n",
    "\n",
    "# This procedure calculates the inorganic carbon chemistry balance\n",
    "# according to the method of Peng et al 1987\n",
    "# The parameters are set in the first few lines\n",
    "#\n",
    "#  ops= 0 ;  output is iteration count\n",
    "#       1 ;            pCO2\n",
    "#       2 ;            pH\n",
    "#       3 ;            [H2CO3]\n",
    "#       4 ;            [HCO3]\n",
    "#       5 ;            [CO3]\n",
    "#       6 ;            satn [co3] : calcite\n",
    "#       7 ;            saturation state: calcite\n",
    "#       8 ;            satn [CO3] : aragonite\n",
    "#       9 ;            saturation state: aragonite\n",
    "#\t10;            Ravelle factor (DIC) calculated from Egleston et al. 2010\n",
    "#\t11;            Alkalinity buffer factor calculated from Egleston et al. 2010\n",
    "\n",
    "    #make sure grids are same size\n",
    "    #make sure rthey years are the same\n",
    "    #extarct the data from the cubes\n",
    "    \n",
    "# from iris import *\n",
    "# from iris.analysis import *\n",
    "# import iris.analysis\n",
    "# from numpy import *\n",
    "# from matplotlib.pyplot import *\n",
    "# from scipy.stats.mstats import *\n",
    "# import iris.plot as iplt\n",
    "# import seawater\n",
    "# import numpy\n",
    "# import iris.quickplot as quickplot\n",
    "# import iris.analysis.stats as istats\n",
    "# temp = iris.load_cube('/home/ph290/tmp/hadgem2es_potential_temperature_historical_regridded.nc').extract(Constraint(depth = 0))\n",
    "# sal = iris.load_cube('/home/ph290/tmp/hadgem2es_salinity_historical_regridded.nc').extract(Constraint(depth = 0))\n",
    "# carb = iris.load_cube('/home/ph290/tmp/hadgem2es_dissolved_inorganic_carbon_historical_regridded.nc').extract(Constraint(depth = 0))\n",
    "# alk = iris.load_cube('/home/ph290/tmp/hadgem2es_total_alkalinity_historical_regridded.nc').extract(Constraint(depth = 0))\n",
    "# import carbchem\n",
    "# co2 = carbchem.carbchem(1,temp.data.fill_value,temp,sal,carb,alk)\n",
    "# T_cube = temp\n",
    "# S_cube = sal\n",
    "# TCO2_cube = carb\n",
    "# TALK_cube = alk  \n",
    "# mdi = temp.data.fill_value\n",
    "\t\n",
    "    t_lat = np.size(T_cube.coord('latitude').points)    \n",
    "    s_lat = np.size(S_cube.coord('latitude').points)\n",
    "    c_lat = np.size(TCO2_cube.coord('latitude').points)\n",
    "    a_lat = np.size(TALK_cube.coord('latitude').points)\n",
    "    lat_test = t_lat == s_lat == c_lat == a_lat\n",
    "\n",
    "    t_lon = np.size(T_cube.coord('longitude').points) \n",
    "    s_lon = np.size(S_cube.coord('longitude').points)\n",
    "    c_lon = np.size(TCO2_cube.coord('longitude').points)\n",
    "    a_lon = np.size(TALK_cube.coord('longitude').points)\n",
    "    lon_test = t_lon == s_lon == c_lon == a_lon\n",
    "\n",
    "    if lat_test and lon_test:\n",
    "        output_cube = T_cube.copy()\n",
    "        T_cube = T_cube\n",
    "        T = T_cube.data.copy()\n",
    "        S = S_cube.data.copy()\n",
    "        TCO2_cube = TCO2_cube/1026.0\n",
    "        # \t\tTCO2 = np.roll(ma.swapaxes(TCO2_cube.data.copy(),0,1),180)\n",
    "        TCO2=TCO2_cube.data.copy()\n",
    "        #NOTE - this is only required here 'cos glodap and WOA are differently ordered - not necessary for other stuff\n",
    "        TALK_cube = TALK_cube/1026.0\n",
    "        # \t\tTALK = np.roll(ma.swapaxes(TALK_cube.data.copy(),0,1),180)\n",
    "        TALK = TALK_cube.data.copy()\n",
    "\n",
    "#         print np.mean(T)\n",
    "#         print np.mean(S)\n",
    "#         print np.mean(TCO2)\n",
    "#         print np.mean(TALK)\n",
    "#         print np.shape(T)\n",
    "#         print np.shape(S)\n",
    "#         print np.shape(TCO2)\n",
    "#         print np.shape(TALK)\n",
    "        msk1=ma.masked_greater_equal(T,mdi-1.0,copy=True)\n",
    "        msk2=ma.masked_greater_equal(S,mdi-1.0,copy=True)\n",
    "        msk3=ma.masked_greater_equal(TCO2,mdi-1.0,copy=True)\n",
    "        msk4=ma.masked_greater_equal(TALK,mdi-1.0,copy=True)\n",
    "\n",
    "        msk=msk1.mask | msk2.mask | msk3.mask | msk4.mask\n",
    "\n",
    "        T[msk]=np.nan\n",
    "        S[msk]=np.nan\n",
    "        TALK[msk]=np.nan\n",
    "        TCO2[msk]=np.nan\n",
    "        # \t\tplt.contourf(T)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(TCO2)\n",
    "        # \t\tplt.show()\n",
    "\n",
    "        # T = np.array([13.74232016,25.0])\n",
    "        # S = np.array([33.74096661,35.0])\n",
    "        # TCO2 = np.array([0.0019863,2.0e-3])\n",
    "        # TALK = np.array([0.00226763,2.2e-3])\n",
    "        # msk = ma.masked_greater_equal(T,mdi-1.0,copy=True)\n",
    "\n",
    "        #create land-sea mask used by sea_msk.mask\n",
    "        salmin = 1.0\n",
    "        S2=np.copy(S)\n",
    "        S2[np.abs(S) < salmin]=salmin\n",
    "\n",
    "        tol = Tl\n",
    "        mxiter = Ni\n",
    "\n",
    "        op_fld = np.empty(T.shape)\n",
    "        op_fld.fill(np.NAN)\n",
    "\n",
    "        #    TB = np.ones(T.shape)\n",
    "        #    TB = 4.106e-4*S2/35.0\n",
    "        TB = np.empty_like(T)\n",
    "        TB = np.multiply(S2,4.106e-4/35.0, TB)\n",
    "        # this boron is from Peng\n",
    "\n",
    "        #convert to Kelvin\n",
    "        TK=np.copy(T[:])\n",
    "        TK += +273.15\n",
    "\n",
    "        alpha_s = np.ones(T.shape)\n",
    "        alpha_s = np.exp( ( -60.2409 + 9345.17/TK  + 23.3585*np.log(TK/100.0) )  + ( 0.023517 - 0.023656*(TK/100.0) + 0.0047036*np.power((TK/100.0),2.0) )*S )\n",
    "\n",
    "        K1 = np.ones(T.shape)\n",
    "        K1 = np.exp( ( -2307.1266/TK + 2.83655  - 1.5529413*np.log(TK) ) - ( 4.0484/TK + 0.20760841 )*np.sqrt(S) + 0.08468345*S - 0.00654208*np.power(S,1.5) + np.log( 1.0 - 0.001005*S ) )\n",
    "\n",
    "        a = np.array([-25.50,-15.82,-29.48,-25.60,-48.76,-46.0])\n",
    "        b = np.array([0.1271,0.0219,0.2324,0.5304,0.5304])\n",
    "        c = np.array([0.0,0.0,0.0026080,0.0036246,0.0,0.0])\n",
    "        d = np.array([-3.08,1.13,(-2.84e-3)/(1.0e-3),-5.13,-11.76,-11.76])\n",
    "        e = np.array([0.0877,0.1475,0.0,0.0794,0.3692,0.3692])\n",
    "\n",
    "        if keyword.iskeyword(Pr):\n",
    "            instance = 0\n",
    "            pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "            K1 = K1*pf\n",
    "\n",
    "        K2 = np.ones(T.shape)\n",
    "        K2 = np.exp( ( -3351.6106/TK - 9.226508 - 0.2005743*np.log(TK) ) - ( 23.9722/TK + 0.106901773 )*np.power(S,0.5) + 0.1130822*S - 0.00846934*np.power(S,1.5) + np.log( 1.0 - 0.001005*S ) )\n",
    "\n",
    "        if keyword.iskeyword(Pr):\n",
    "            instance = 1\n",
    "            pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "            K2 = K2*pf\n",
    "\n",
    "        KB = np.ones(T.shape)\n",
    "        KB = np.exp( ( -8966.90 - 2890.53*np.power(S,0.5) - 77.942*S + 1.728*np.power(S,1.5)- 0.0996*np.power(S,2.0) )/TK + ( 148.0248 + 137.1942*np.power(S,0.5) + 1.62142*S ) - ( 24.4344 + 25.085*np.power(S,0.5) + 0.2474*S )*np.log(TK) + 0.053105*(np.power(S,0.5))*TK )\n",
    "\n",
    "        if keyword.iskeyword(Pr):\n",
    "            instance = 2\n",
    "            pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "            KB = KB*pf\n",
    "\n",
    "        KW = np.ones(T.shape)\n",
    "        KW = np.exp( ( -13847.26/TK + 148.96502 - 23.6521*np.log(TK) ) + ( 118.67/TK - 5.977 + 1.0495*np.log(TK) )*np.power(S,0.5) - 0.01615*S )\n",
    "\n",
    "        if keyword.iskeyword(Pr):\n",
    "            instance = 3\n",
    "            pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "            KW = KW*pf\n",
    "\n",
    "        if ( op_swtch >= 6 or op_swtch <= 9 ):\n",
    "            ca_conc = np.ones(T.shape)\n",
    "            ca_conc = 0.01028*S2/35.0\n",
    "\n",
    "        if ( op_swtch == 6 or op_swtch == 7 ):\n",
    "            K_SP_C = np.ones(T.shape)\n",
    "            K_SP_C = np.power(10.0,( ( -171.9065 - 0.077993*TK + 2839.319/TK + 71.595*np.log10(TK) ) + ( -0.77712 + 0.0028426*TK + 178.34/TK )*np.power(S,0.5) - 0.07711*S+ 0.0041249*np.power(S,1.5) ))\n",
    "            if keyword.iskeyword(Pr):\n",
    "                instance = 4\n",
    "                pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "                K_SP_C = K_SP_C*pf\n",
    "\n",
    "\n",
    "        if ( op_swtch == 8 or op_swtch == 9 ):\n",
    "            K_SP_A = np.ones(T.shape)\n",
    "            K_SP_A = np.power(10,( ( -171.945 - 0.077993*TK + 2903.293/TK + 71.595*np.log10(TK) ) + ( -0.068393 + 0.0017276*TK + 88.135/TK )*np.power(S,0.5) - 0.10018*S + 0.0059415*np.power(S,1.5) ))\n",
    "            if keyword.iskeyword(Pr):\n",
    "                instance = 5\n",
    "                pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "                K_SP_A = K_SP_A*pf\n",
    "\n",
    "\n",
    "        # Get first estimate for H+ concentration.\n",
    "\n",
    "        AC, AW, AB, aH, count = carbiter(T, TCO2, TALK, TB, msk, tol, mxiter, K1, K2, KB, KW)\n",
    "\n",
    "        # \t\tplt.contourf(aH)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(AC)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(AW)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(aH)\n",
    "        # \t\tplt.show()\n",
    "\n",
    "        # now we have aH we can calculate...\n",
    "        denom = np.zeros(T.shape)\n",
    "        H2CO3 = np.zeros(T.shape)\n",
    "        HCO3 = np.zeros(T.shape)\n",
    "        CO3 = np.zeros(T.shape)\n",
    "        pH = np.zeros(T.shape)\n",
    "        pCO2 = np.zeros(T.shape)\n",
    "        if ( op_swtch == 6 or op_swtch == 7 ):\n",
    "            sat_CO3_C = np.zeros(T.shape)\n",
    "        if ( op_swtch == 7 ):\n",
    "            sat_stat_C = np.zeros(T.shape)\n",
    "        if ( op_swtch == 8 or op_swtch == 9 ):\n",
    "            sat_CO3_A = np.zeros(T.shape)\n",
    "        if ( op_swtch == 9 ):\n",
    "            sat_stat_A = np.zeros(T.shape)\n",
    "\n",
    "        denom = np.power(aH,2.0) + K1*aH + K1*K2\n",
    "        H2CO3 = TCO2*np.power(aH,2.0)/denom\n",
    "        HCO3 = TCO2*K1*aH/denom\n",
    "        CO3 = TCO2*K1*K2/denom\n",
    "        # \t\tplt.contourf(K1)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(aH) -no\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(denom) -no\n",
    "        # \t\tplt.show()\n",
    "\n",
    "        pH = -np.log10(aH)\n",
    "        pCO2 = H2CO3/alpha_s\n",
    "\n",
    "        if ( op_swtch == 6 or op_swtch == 7 ):\n",
    "            sat_CO3_C = K_SP_C/ca_conc\n",
    "            if ( op_swtch == 7 ):\n",
    "                sat_stat_C = CO3/sat_CO3_C\n",
    "\n",
    "        if ( op_swtch == 8 or op_swtch == 9 ):\n",
    "            sat_CO3_A = K_SP_A/ca_conc\n",
    "            if ( op_swtch == 9 ):\n",
    "                sat_stat_A = CO3/sat_CO3_A\n",
    "\n",
    "        TALKc=+HCO3+2*(CO3)\n",
    "        var1=HCO3\n",
    "        DIC_buffer=HCO3\n",
    "        ALK_buffer=HCO3\n",
    "\n",
    "        var1=HCO3+4*(CO3)+((aH*AB)/(KB+aH))-AW\n",
    "\n",
    "        DIC_buffer=TCO2-((TALKc*TALKc)/var1)\n",
    "\n",
    "        ALK_buffer=((TALKc*TALKc)-TCO2*var1)/TALKc\n",
    "\n",
    "        output_cube = output_cube*0.0+np.nan\n",
    "        if ( op_swtch == 0 ):\n",
    "            op_fld = np.zeros(T.shape)\n",
    "            op_fld = count\n",
    "        elif ( op_swtch == 1 ):\n",
    "            print np.mean(pCO2)\n",
    "            output_cube.data = pCO2*1.0e6\n",
    "            output_cube.standard_name = 'surface_partial_pressure_of_carbon_dioxide_in_sea_water'\n",
    "            output_cube.long_name = 'CO2 concentration'\n",
    "            output_cube.units = 'uatm'\n",
    "        elif ( op_swtch == 2 ):\n",
    "            output_cube.data = pH\n",
    "            output_cube.standard_name = 'sea_water_ph_reported_on_total_scale'\n",
    "            output_cube.long_name = 'pH'\n",
    "            output_cube.units = '1'\n",
    "        elif ( op_swtch == 3 ):\n",
    "            output_cube.data = H2CO3\n",
    "        elif ( op_swtch == 4 ):\n",
    "            output_cube.data = HCO3\n",
    "        elif ( op_swtch == 5 ):\n",
    "            output_cube.data = CO3\n",
    "        elif ( op_swtch == 6 ):\n",
    "            output_cube.data = sat_CO3_C\n",
    "        elif ( op_swtch == 7 ):\n",
    "            output_cube.data = sat_stat_C\n",
    "        elif ( op_swtch == 8 ):\n",
    "            output_cube.data = sat_CO3_A\n",
    "        elif ( op_swtch == 9 ):\n",
    "            output_cube.data = sat_stat_A\n",
    "        elif ( op_swtch == 10 ):\n",
    "            output_cube.data = TCO2/DIC_buffer\n",
    "        elif ( op_swtch == 11 ):\n",
    "            output_cube.data = ALK_buffer*1000.0\n",
    "\n",
    "        return output_cube\n",
    "\n",
    "\n",
    "'''\n",
    "test-data\n",
    "'''\n",
    "\n",
    "# def main():\n",
    "# mdi=-999.0\n",
    "# sizing=(500,500)\n",
    "# T = np.empty(sizing)\n",
    "# S = np.empty(sizing)\n",
    "# TCO2 = np.empty(sizing)\n",
    "# TALK = np.empty(sizing)\n",
    "# T.fill(10.0)\n",
    "# S.fill(35.0)\n",
    "# TCO2.fill(0.0020)\n",
    "# TALK.fill(0.0022)\n",
    "# T[0,0]=mdi\n",
    "# S[2,3]=mdi\n",
    "# S[0,0]=0.5\n",
    "# TALK[2,3]=mdi\n",
    "# TCO2[2,3]=mdi\n",
    "    \n",
    "#     print carbchem(1,mdi,T,S,TCO2,TALK)\n",
    "\n",
    "# import cProfile\n",
    "# if __name__ == '__main__':\n",
    "#     x=cProfile.run('main()')\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMIP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1 = '/data/BatCaveNAS/ph290/cmip6/regridded/'\n",
    "# directory2 = '/data/BatCaveNAS/ph290/cmip6/processed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CESM2', 'CESM2-WACCM', 'CNRM-ESM2-1', 'CanESM5', 'EC-Earth3-Veg', 'GFDL-CM4', 'IPSL-CM6A-LR', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NESM3']\n",
      "talk\n",
      "['CNRM-ESM2-1', 'GFDL-CM4', 'MPI-ESM1-2-HR', 'CESM2', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC-ES2L']\n",
      "dissic\n",
      "['CNRM-ESM2-1', 'GFDL-CM4', 'MPI-ESM1-2-HR', 'CESM2', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC-ES2L']\n",
      "so\n",
      "['CNRM-ESM2-1', 'GFDL-CM4', 'MPI-ESM1-2-HR', 'CESM2', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC-ES2L']\n",
      "fgco2\n",
      "['CNRM-ESM2-1', 'GFDL-CM4', 'MPI-ESM1-2-HR', 'CESM2', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC-ES2L']\n",
      "['CNRM-ESM2-1', 'GFDL-CM4', 'MPI-ESM1-2-HR', 'CESM2', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC-ES2L']\n"
     ]
    }
   ],
   "source": [
    "variables = ['tos','talk','dissic','so','fgco2']\n",
    "\n",
    "gn_models = []\n",
    "gr_models = []\n",
    "gn_models = np.unique(list(model_names_var(directory1,variables[0],'gn')))\n",
    "gr_models = np.unique(list(model_names_var(directory1,variables[0],'gr')))\n",
    "                               \n",
    "models_cmip6 = list(np.unique(list(gn_models) + list(gr_models)))\n",
    "print models_cmip6\n",
    "#check to see if all variables available\n",
    "for var in variables[1::]:\n",
    "    gn_models = []\n",
    "    gr_models = []\n",
    "    gn_models = np.unique(list(model_names_var(directory1,var,'gn')))\n",
    "    gr_models = np.unique(list(model_names_var(directory1,var,'gr')))\n",
    "    models1 = list(np.unique(list(gn_models) + list(gr_models)))\n",
    "    models_cmip6 = list(set(models_cmip6).intersection(models1))\n",
    "    print var\n",
    "    print models_cmip6\n",
    "#     print var\n",
    "#     print models_cmip6\n",
    "\n",
    "print models_cmip6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def populate_data_dict(directory1,data_dict,model,grid,mask_seaice):\n",
    "    test=False\n",
    "    if mask_seaice:\n",
    "        test = os.path.isfile('/disk2/ph290/pickle/'+model+'_revelle_mask_seaice.pickle')\n",
    "    else:\n",
    "        test = os.path.isfile('/disk2/ph290/pickle/'+model+'_revelle.pickle')\n",
    "    if not test:\n",
    "        lon_west,lon_east,lat_south,lat_north = -180,180,-90,-45\n",
    "        print directory1+model+'_dissic_ssp585_*_'+grid+'_regridded.nc'\n",
    "        file = glob.glob(directory1+model+'_dissic_ssp585_*_'+grid+'_regridded.nc')[0]\n",
    "\n",
    "        file = file.split('/')[-1]\n",
    "        ensemble = file.split('_')[3]\n",
    "        start_yr,end_yr = 2015,2035\n",
    "        start_yr2,end_yr2 = 2079,2099\n",
    "\n",
    "        print 'reading in data'\n",
    "    #     cube_tos = extract_region(mask_cube_if_no_mask(iris.load_cube(directory1+model+'_tos_ssp585_'+ensemble+'_'+grid+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "    #     cube_sos = extract_region(mask_cube_if_no_mask(iris.load_cube(directory1+model+'_so*_ssp585_'+ensemble+'_'+grid+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "    #     cube_talk = extract_region(mask_cube_if_no_mask(iris.load_cube(directory1+model+'_talk_ssp585_'+ensemble+'_'+grid+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "    #     cube_dissic = extract_region(mask_cube_if_no_mask(iris.load_cube(directory1+model+'_dissic_ssp585_'+ensemble+'_'+grid+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "    #     cube_fgco2 = extract_region(mask_cube_if_no_mask(iris.load_cube(directory1+model+'_fgco2_ssp585_'+ensemble+'_'+grid+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "        print 'reading in tos'\n",
    "        cube_tos = mask_cube_if_no_mask(extract_region(iris.load_cube(directory1+model+'_tos_ssp585_'+ensemble+'_'+grid+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_tos, 'time', name='year')\n",
    "        cube_tos = cube_tos[np.where(cube_tos.coord('year').points <= 2100)]\n",
    "        print 'reading in sos'\n",
    "        cube_sos = mask_cube_if_no_mask(extract_region(iris.load_cube(directory1+model+'_sos_ssp585_'+ensemble+'_'+grid+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_sos, 'time', name='year')\n",
    "        cube_sos = cube_sos[np.where(cube_sos.coord('year').points <= 2100)]\n",
    "        print 'reading in talk'\n",
    "        cube_talk = mask_cube_if_no_mask(extract_region(iris.load_cube(directory1+model+'_talk_ssp585_'+ensemble+'_'+grid+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_talk, 'time', name='year')\n",
    "        cube_talk = cube_talk[np.where(cube_talk.coord('year').points <= 2100)]\n",
    "        print 'reading in dissic'\n",
    "        cube_dissic = mask_cube_if_no_mask(extract_region(iris.load_cube(directory1+model+'_dissic_ssp585_'+ensemble+'_'+grid+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_dissic, 'time', name='year')\n",
    "        cube_dissic = cube_dissic[np.where(cube_dissic.coord('year').points <= 2100)]\n",
    "        print 'reading in fgco2'\n",
    "        cube_fgco2 = mask_cube_if_no_mask(extract_region(iris.load_cube(directory1+model+'_fgco2_ssp585_'+ensemble+'_'+grid+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_fgco2, 'time', name='year')\n",
    "        cube_fgco2 = cube_fgco2[np.where(cube_fgco2.coord('year').points <= 2100)]\n",
    "        print 'reading in data complete'\n",
    "\n",
    "        cube_sos = collapse_depth(cube_sos)\n",
    "        cube_talk = collapse_depth(cube_talk)\n",
    "        cube_dissic = collapse_depth(cube_dissic)\n",
    "\n",
    "        if mask_seaice:\n",
    "            cube_siconc = mask_cube_if_no_mask(extract_region(iris.load_cube(directory1+model+'_siconc_ssp585_'+ensemble+'_'+grid+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "            max_seaice = cube_siconc.collapsed('time',iris.analysis.MAX)\n",
    "\n",
    "            cube_tos = mask_where_seaice(cube_tos,max_seaice)\n",
    "            cube_sos = mask_where_seaice(cube_sos,max_seaice)\n",
    "            cube_talk = mask_where_seaice(cube_talk,max_seaice)\n",
    "            cube_dissic = mask_where_seaice(cube_dissic,max_seaice)\n",
    "            cube_fgco2 = mask_where_seaice(cube_fgco2,max_seaice)\n",
    "        \n",
    "        if np.max(cube_tos.data) < 200:\n",
    "            cube_tos += 273.15\n",
    "\n",
    "        print 'starting revelle calculation'\n",
    "        if np.shape(cube_talk)[0] == np.shape(cube_tos)[0] == np.shape(cube_sos)[0] == np.shape(cube_dissic)[0]: #i.e. checking all are monthly means:\n",
    "            revelle_factor = carbchem_revelle(10,9.99e9,cube_tos-273.15,cube_sos,cube_dissic,cube_talk)\n",
    "        else:\n",
    "            cube_tos = year_mean(cube_tos)\n",
    "            cube_sos = year_mean(cube_sos)\n",
    "            cube_dissic = year_mean(cube_dissic)\n",
    "            cube_talk = year_mean(cube_talk)\n",
    "            revelle_factor = carbchem_revelle(10,9.99e9,cube_tos-273.15,cube_sos,cube_dissic,cube_talk)\n",
    "        print 'ending revelle calculation'\n",
    "\n",
    "\n",
    "        cube_tos_start = avg_years(cube_tos,start_yr,end_yr)\n",
    "        cube_sos_start = avg_years(cube_sos,start_yr,end_yr)\n",
    "        cube_talk_start = avg_years(cube_talk,start_yr,end_yr)\n",
    "        cube_dissic_start = avg_years(cube_dissic,start_yr,end_yr)\n",
    "        revelle_factor_start = avg_years(revelle_factor,start_yr,end_yr)\n",
    "\n",
    "        cube_fgco2_area_avg = area_avg(cube_fgco2)\n",
    "        cube_fgco2_area_avg_start = avg_years(cube_fgco2_area_avg,start_yr,end_yr)\n",
    "        cube_fgco2_area_avg_end = avg_years(cube_fgco2_area_avg,start_yr2,end_yr2)\n",
    "        cube_fgco2_area_avg_change = cube_fgco2_area_avg_end - cube_fgco2_area_avg_start\n",
    "\n",
    "        cube_fgco2_area_sum = area_sum(cube_fgco2)\n",
    "        cube_fgco2_area_sum_start = avg_years(cube_fgco2_area_sum,start_yr,end_yr)\n",
    "        cube_fgco2_area_sum_end = avg_years(cube_fgco2_area_sum,start_yr2,end_yr2)\n",
    "        cube_fgco2_area_sum_change = cube_fgco2_area_sum_end - cube_fgco2_area_sum_start\n",
    "\n",
    "        cube_fgco2_avg_yrs = avg_years(cube_fgco2,start_yr,end_yr)\n",
    "        cube_tos_avg_yrs = avg_years(cube_tos,start_yr2,end_yr2)\n",
    "        cube_sos_avg_yrs = avg_years(cube_sos,start_yr2,end_yr2)\n",
    "        cube_dissic_avg_yrs = avg_years(cube_dissic,start_yr2,end_yr2)\n",
    "        cube_talk_avg_yrs = avg_years(cube_talk,start_yr2,end_yr2)\n",
    "        cube_fgco2_avg_yrs = avg_years(cube_fgco2,start_yr2,end_yr2)\n",
    "        revelle_factor_start_avg_yrs = area_avg(revelle_factor_start)\n",
    "\n",
    "\n",
    "        if mask_seaice:\n",
    "            with open('/disk2/ph290/pickle/'+model+'_revelle_mask_seaice.pickle','w') as f:\n",
    "                pickle.dump([cube_tos_start,cube_sos_start,cube_dissic_start,cube_talk_start,cube_fgco2_avg_yrs,cube_tos_avg_yrs,cube_sos_avg_yrs,cube_dissic_avg_yrs,cube_talk_avg_yrs,cube_fgco2_avg_yrs,revelle_factor_start,revelle_factor_start_avg_yrs,cube_fgco2_area_avg_change,cube_fgco2_area_avg_change,cube_fgco2_area_sum_change], f)\n",
    "        else:\n",
    "            with open('/disk2/ph290/pickle/'+model+'_revelle.pickle', 'w') as f:\n",
    "                pickle.dump([cube_tos_start,cube_sos_start,cube_dissic_start,cube_talk_start,cube_fgco2_avg_yrs,cube_tos_avg_yrs,cube_sos_avg_yrs,cube_dissic_avg_yrs,cube_talk_avg_yrs,cube_fgco2_avg_yrs,revelle_factor_start,revelle_factor_start_avg_yrs,cube_fgco2_area_avg_change,cube_fgco2_area_avg_change,cube_fgco2_area_sum_change], f)\n",
    "\n",
    "    else:\n",
    "        print 'model values already calculated'\n",
    "        if mask_seaice:\n",
    "            with open('/disk2/ph290/pickle/'+model+'_revelle_mask_seaice.pickle','r') as f:\n",
    "                [cube_tos_start,cube_sos_start,cube_dissic_start,cube_talk_start,cube_fgco2_avg_yrs,cube_tos_avg_yrs,cube_sos_avg_yrs,cube_dissic_avg_yrs,cube_talk_avg_yrs,cube_fgco2_avg_yrs,revelle_factor_start,revelle_factor_start_avg_yrs,cube_fgco2_area_avg_change,cube_fgco2_area_avg_change,cube_fgco2_area_sum_change] = pickle.load(f)\n",
    "        else:\n",
    "            with open('/disk2/ph290/pickle/'+model+'_revelle.pickle','r') as f:\n",
    "                [cube_tos_start,cube_sos_start,cube_dissic_start,cube_talk_start,cube_fgco2_avg_yrs,cube_tos_avg_yrs,cube_sos_avg_yrs,cube_dissic_avg_yrs,cube_talk_avg_yrs,cube_fgco2_avg_yrs,revelle_factor_start,revelle_factor_start_avg_yrs,cube_fgco2_area_avg_change,cube_fgco2_area_avg_change,cube_fgco2_area_sum_change] = pickle.load(f)\n",
    "\n",
    "    if mask_seaice:\n",
    "        seaice_label = 'seaice_masked'\n",
    "    else:\n",
    "        seaice_label = 'seaice_not_masked'\n",
    "        \n",
    "    data_dict[seaice_label][model]['tos_map_start'] = cube_tos_start\n",
    "    data_dict[seaice_label][model]['sos_map_start'] = cube_sos_start\n",
    "    data_dict[seaice_label][model]['dissic_map_start'] = cube_dissic_start\n",
    "    data_dict[seaice_label][model]['talk_map_start'] = cube_talk_start\n",
    "    data_dict[seaice_label][model]['fgco2_map_start'] = cube_fgco2_avg_yrs\n",
    "    data_dict[seaice_label][model]['tos_map_end'] = cube_tos_avg_yrs\n",
    "    data_dict[seaice_label][model]['sos_map_end'] = cube_sos_avg_yrs\n",
    "    data_dict[seaice_label][model]['dissic_map_end'] = cube_dissic_avg_yrs\n",
    "    data_dict[seaice_label][model]['talk_map_end'] = cube_talk_avg_yrs\n",
    "    data_dict[seaice_label][model]['fgco2_map_end'] = cube_fgco2_avg_yrs\n",
    "    data_dict[seaice_label][model]['revelle_factor_map'] = revelle_factor_start\n",
    "    data_dict[seaice_label][model]['revelle_factor_mean'] = revelle_factor_start_avg_yrs\n",
    "    data_dict[seaice_label][model]['fgco2_change'] = cube_fgco2_area_avg_change\n",
    "    data_dict[seaice_label][model]['fgco2_change_mean'] = cube_fgco2_area_avg_change\n",
    "    data_dict[seaice_label][model]['fgco2_change_sum'] = cube_fgco2_area_sum_change\n",
    "    data_dict[seaice_label][model]['label'] = 'cmip6'\n",
    "       \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNRM-ESM2-1', 'GFDL-CM4', 'MPI-ESM1-2-HR', 'CESM2', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC-ES2L']\n",
      "CNRM-ESM2-1\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "GFDL-CM4\n",
      "gn\n",
      " tos: False  sos: False  dissic: False  talk: False\n",
      "gr\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with gr data\n",
      "MPI-ESM1-2-HR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "CESM2\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "CanESM5\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "IPSL-CM6A-LR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "MIROC-ES2L\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n"
     ]
    }
   ],
   "source": [
    "print models_cmip6\n",
    "data_dict={}\n",
    "mask_seaice = False\n",
    "if mask_seaice:\n",
    "    seaice_label = 'seaice_masked'\n",
    "else:\n",
    "    seaice_label = 'seaice_not_masked'\n",
    "data_dict[seaice_label]={}\n",
    "for model in models_cmip6:\n",
    "    print model\n",
    "    if mask_seaice:\n",
    "        data_dict['seaice_masked'][model]={}\n",
    "    else:\n",
    "        data_dict['seaice_not_masked'][model]={}\n",
    "    grid = 'gn'\n",
    "#     print os.listdir(directory1)\n",
    "    test1 = len(glob.glob(directory1+model+'_tos_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    test2 = len(glob.glob(directory1+model+'_sos_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    test3 = len(glob.glob(directory1+model+'_dissic_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    test4 = len(glob.glob(directory1+model+'_talk_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    if mask_seaice:\n",
    "        test5= len(glob.glob(directory1+model+'_siconc_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    else:\n",
    "        test5 = test4\n",
    "    print grid\n",
    "    if mask_seaice:\n",
    "        print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4,' siconc:',test5\n",
    "    else:\n",
    "        print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4\n",
    "    if test1 & test2 & test3 & test4 & test5:\n",
    "        data_dict = populate_data_dict(directory1,data_dict,model,grid,mask_seaice)\n",
    "        print 'populated data_dict with '+grid+' data'\n",
    "    else:\n",
    "        grid = 'gr'\n",
    "        test1 = len(glob.glob(directory1+model+'_tos_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        test2 = len(glob.glob(directory1+model+'_sos_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        test3 = len(glob.glob(directory1+model+'_dissic_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        test4 = len(glob.glob(directory1+model+'_talk_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        if mask_seaice:\n",
    "            test5= len(glob.glob(directory1+model+'_siconc_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        else:\n",
    "            test5 = test4\n",
    "        print grid\n",
    "        if mask_seaice:\n",
    "            print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4,' siconc:',test5\n",
    "        else:\n",
    "            print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4\n",
    "        if test1 & test2 & test3 & test4 & test5:\n",
    "            data_dict = populate_data_dict(directory1,data_dict,model,grid,mask_seaice)\n",
    "            print 'populated data_dict with '+grid+' data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNRM-ESM2-1', 'GFDL-CM4', 'MPI-ESM1-2-HR', 'CESM2', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC-ES2L']\n",
      "CNRM-ESM2-1\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  siconc: False\n",
      "gr\n",
      " tos: False  sos: False  dissic: False  talk: False  siconc: False\n",
      "GFDL-CM4\n",
      "gn\n",
      " tos: False  sos: False  dissic: False  talk: False  siconc: True\n",
      "gr\n",
      " tos: True  sos: True  dissic: True  talk: True  siconc: True\n",
      "model values already calculated\n",
      "populated data_dict with gr data\n",
      "MPI-ESM1-2-HR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  siconc: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "CESM2\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  siconc: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "CanESM5\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  siconc: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "IPSL-CM6A-LR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  siconc: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n",
      "MIROC-ES2L\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  siconc: True\n",
      "model values already calculated\n",
      "populated data_dict with gn data\n"
     ]
    }
   ],
   "source": [
    "print models_cmip6\n",
    "mask_seaice = True\n",
    "if mask_seaice:\n",
    "    seaice_label = 'seaice_masked'\n",
    "else:\n",
    "    seaice_label = 'seaice_not_masked'\n",
    "data_dict[seaice_label]={}\n",
    "for model in models_cmip6:\n",
    "    print model\n",
    "    if mask_seaice:\n",
    "        data_dict['seaice_masked'][model]={}\n",
    "    else:\n",
    "        data_dict['seaice_not_masked'][model]={}\n",
    "    grid = 'gn'\n",
    "#     print os.listdir(directory1)\n",
    "    test1 = len(glob.glob(directory1+model+'_tos_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    test2 = len(glob.glob(directory1+model+'_sos_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    test3 = len(glob.glob(directory1+model+'_dissic_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    test4 = len(glob.glob(directory1+model+'_talk_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    if mask_seaice:\n",
    "        test5= len(glob.glob(directory1+model+'_siconc_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "    else:\n",
    "        test5 = test4\n",
    "    print grid\n",
    "    if mask_seaice:\n",
    "        print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4,' siconc:',test5\n",
    "    else:\n",
    "        print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4\n",
    "    if test1 & test2 & test3 & test4 & test5:\n",
    "        data_dict = populate_data_dict(directory1,data_dict,model,grid,mask_seaice)\n",
    "        print 'populated data_dict with '+grid+' data'\n",
    "    else:\n",
    "        grid = 'gr'\n",
    "        test1 = len(glob.glob(directory1+model+'_tos_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        test2 = len(glob.glob(directory1+model+'_sos_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        test3 = len(glob.glob(directory1+model+'_dissic_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        test4 = len(glob.glob(directory1+model+'_talk_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        if mask_seaice:\n",
    "            test5= len(glob.glob(directory1+model+'_siconc_ssp585_*_'+grid+'_regridded.nc')) == 1\n",
    "        else:\n",
    "            test5 = test4\n",
    "        print grid\n",
    "        if mask_seaice:\n",
    "            print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4,' siconc:',test5\n",
    "        else:\n",
    "            print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4\n",
    "        if test1 & test2 & test3 & test4 & test5:\n",
    "            data_dict = populate_data_dict(directory1,data_dict,model,grid,mask_seaice)\n",
    "            print 'populated data_dict with '+grid+' data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lon_west,lon_east,lat_south,lat_north = -180,180,-90,-45\n",
    "# model = 'CanESM5'\n",
    "# ensemble = 'r1i1p1f1'\n",
    "# grid = 'gn'\n",
    "# cube_tos = mask_cube_if_no_mask(extract_region(iris.load_cube(directory1+model+'_tos_ssp585_'+ensemble+'_'+grid+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "# cube_siconc = mask_cube_if_no_mask(extract_region(iris.load_cube(directory1+model+'_siconc_ssp585_'+ensemble+'_'+grid+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "\n",
    "# max_seaice = cube_siconc.collapsed('time',iris.analysis.MAX)\n",
    "# cube_tos_2 = cube_tos.copy()\n",
    "\n",
    "# print np.shape(max_seaice.data)\n",
    "# print np.shape(cube_tos_2)\n",
    "# sic_mask = np.tile(max_seaice.data,[np.shape(cube_tos_2)[0],1,1])\n",
    "# print np.shape(sic_mask)\n",
    "# cube_tos_2.data.mask[np.where(sic_mask > 0)] = True\n",
    "\n",
    "# qplt.contourf(cube_tos_2[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMIP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory2 = '/data/BatCaveNAS/ph290/cmip5/regridded/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACCESS1-0', 'ACCESS1-3', 'CCSM4', 'CESM1-BGC', 'CESM1-CAM5', 'CMCC-CESM', 'CMCC-CM', 'CMCC-CMS', 'CNRM-CM5', 'CSIRO-Mk3-6-0', 'CanESM2', 'EC-EARTH', 'GFDL-CM3', 'GFDL-ESM2G', 'GFDL-ESM2M', 'GISS-E2-H', 'GISS-E2-H-CC', 'GISS-E2-R', 'GISS-E2-R-CC', 'HadGEM2-AO', 'HadGEM2-CC', 'HadGEM2-ES', 'IPSL-CM5A-LR', 'IPSL-CM5A-MR', 'IPSL-CM5B-LR', 'MIROC-ESM', 'MIROC-ESM-CHEM', 'MIROC5', 'MPI-ESM-LR', 'MPI-ESM-MR', 'MRI-CGCM3', 'NorESM1-M', 'NorESM1-ME', 'bcc-csm1-1', 'bcc-csm1-1-m', 'inmcm4']\n",
      "talk\n",
      "talk\n",
      "['IPSL-CM5A-LR', 'MPI-ESM-LR', 'CNRM-CM5', 'CanESM2', 'IPSL-CM5B-LR', 'NorESM1-ME', 'HadGEM2-ES', 'GFDL-ESM2G', 'HadGEM2-CC', 'CMCC-CESM', 'MPI-ESM-MR', 'GFDL-ESM2M', 'IPSL-CM5A-MR', 'CESM1-BGC']\n",
      "dissic\n",
      "dissic\n",
      "['IPSL-CM5A-LR', 'MPI-ESM-LR', 'CNRM-CM5', 'CanESM2', 'IPSL-CM5B-LR', 'HadGEM2-ES', 'GFDL-ESM2G', 'HadGEM2-CC', 'CMCC-CESM', 'MPI-ESM-MR', 'GFDL-ESM2M', 'IPSL-CM5A-MR', 'CESM1-BGC']\n",
      "sos\n",
      "sos\n",
      "['IPSL-CM5A-LR', 'MPI-ESM-LR', 'CNRM-CM5', 'CanESM2', 'IPSL-CM5B-LR', 'HadGEM2-ES', 'GFDL-ESM2G', 'HadGEM2-CC', 'CMCC-CESM', 'MPI-ESM-MR', 'GFDL-ESM2M', 'IPSL-CM5A-MR', 'CESM1-BGC']\n",
      "fgco2\n",
      "fgco2\n",
      "['IPSL-CM5A-LR', 'MPI-ESM-LR', 'CNRM-CM5', 'CanESM2', 'IPSL-CM5B-LR', 'HadGEM2-ES', 'GFDL-ESM2G', 'HadGEM2-CC', 'CMCC-CESM', 'MPI-ESM-MR', 'GFDL-ESM2M', 'IPSL-CM5A-MR', 'CESM1-BGC']\n",
      "sic\n",
      "sic\n",
      "['IPSL-CM5A-LR', 'MPI-ESM-LR', 'CNRM-CM5', 'CanESM2', 'IPSL-CM5B-LR', 'HadGEM2-ES', 'GFDL-ESM2G', 'HadGEM2-CC', 'CMCC-CESM', 'MPI-ESM-MR', 'GFDL-ESM2M', 'IPSL-CM5A-MR', 'CESM1-BGC']\n",
      "['IPSL-CM5A-LR', 'MPI-ESM-LR', 'CNRM-CM5', 'CanESM2', 'IPSL-CM5B-LR', 'HadGEM2-ES', 'GFDL-ESM2G', 'HadGEM2-CC', 'CMCC-CESM', 'MPI-ESM-MR', 'GFDL-ESM2M', 'IPSL-CM5A-MR', 'CESM1-BGC']\n"
     ]
    }
   ],
   "source": [
    "variables = ['tos','talk','dissic','sos','fgco2','sic']\n",
    "\n",
    "models = []\n",
    "models = np.unique(list(model_names_var_cmip5(directory2,variables[0])))\n",
    "                               \n",
    "models_cmip5 = list(models)\n",
    "print models_cmip5\n",
    "#check to see if all variables available\n",
    "for var in variables[1::]:\n",
    "    print var\n",
    "    models = np.unique(list(model_names_var_cmip5(directory2,var)))\n",
    "    models1 = list(models)\n",
    "    models_cmip5 = list(set(models_cmip5).intersection(models1))\n",
    "    print var\n",
    "    print models_cmip5\n",
    "#     print var\n",
    "#     print models_cmip\n",
    "\n",
    "print models_cmip5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_data_dict_cmip5(directory2,data_dict,model,grid,mask_seaice):\n",
    "    test=False\n",
    "    if mask_seaice:\n",
    "        test = os.path.isfile('/disk2/ph290/pickle/'+model+'_revelle_mask_seaice_cmip5.pickle')\n",
    "    else:\n",
    "        test = os.path.isfile('/disk2/ph290/pickle/'+model+'_revelle_cmip5.pickle')\n",
    "    if not test:\n",
    "        lon_west,lon_east,lat_south,lat_north = -180,180,-90,-45\n",
    "        print directory2+model+'_dissic_rcp85_*_regridded.nc'\n",
    "        file = glob.glob(directory2+model+'_dissic_rcp85_*_regridded.nc')[0]\n",
    "\n",
    "        file = file.split('/')[-1]\n",
    "        ensemble = file.split('_')[3]\n",
    "        start_yr,end_yr = 2015,2035\n",
    "        start_yr2,end_yr2 = 2079,2099\n",
    "\n",
    "        print 'reading in data'\n",
    "    #     cube_tos = extract_region(mask_cube_if_no_mask(iris.load_cube(directory2+model+'_tos_rcp85_'+ensemble+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "    #     cube_sos = extract_region(mask_cube_if_no_mask(iris.load_cube(directory2+model+'_so*_rcp85_'+ensemble+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "    #     cube_talk = extract_region(mask_cube_if_no_mask(iris.load_cube(directory2+model+'_talk_rcp85_'+ensemble+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "    #     cube_dissic = extract_region(mask_cube_if_no_mask(iris.load_cube(directory2+model+'_dissic_rcp85_'+ensemble+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "    #     cube_fgco2 = extract_region(mask_cube_if_no_mask(iris.load_cube(directory2+model+'_fgco2_rcp85_'+ensemble+'_regridded.nc')),lon_west,lon_east,lat_south,lat_north)\n",
    "        print 'reading in tos'\n",
    "        cube_tos = mask_cube_if_no_mask(extract_region(iris.load_cube(directory2+model+'_tos_rcp85_'+ensemble+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_tos, 'time', name='year')\n",
    "        cube_tos = cube_tos[np.where(cube_tos.coord('year').points <= 2100)]\n",
    "        print 'reading in sos'\n",
    "        cube_sos = mask_cube_if_no_mask(extract_region(iris.load_cube(directory2+model+'_sos_rcp85_'+ensemble+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_sos, 'time', name='year')\n",
    "        cube_sos = cube_sos[np.where(cube_sos.coord('year').points <= 2100)]\n",
    "        print 'reading in talk'\n",
    "        cube_talk = mask_cube_if_no_mask(extract_region(iris.load_cube(directory2+model+'_talk_rcp85_'+ensemble+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_talk, 'time', name='year')\n",
    "        cube_talk = cube_talk[np.where(cube_talk.coord('year').points <= 2100)]\n",
    "        print 'reading in dissic'\n",
    "        cube_dissic = mask_cube_if_no_mask(extract_region(iris.load_cube(directory2+model+'_dissic_rcp85_'+ensemble+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_dissic, 'time', name='year')\n",
    "        cube_dissic = cube_dissic[np.where(cube_dissic.coord('year').points <= 2100)]\n",
    "        print 'reading in fgco2'\n",
    "        cube_fgco2 = mask_cube_if_no_mask(extract_region(iris.load_cube(directory2+model+'_fgco2_rcp85_'+ensemble+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "        iris.coord_categorisation.add_year(cube_fgco2, 'time', name='year')\n",
    "        cube_fgco2 = cube_fgco2[np.where(cube_fgco2.coord('year').points <= 2100)]\n",
    "        print 'reading in data complete'\n",
    "\n",
    "        cube_sos = collapse_depth(cube_sos)\n",
    "        cube_talk = collapse_depth(cube_talk)\n",
    "        cube_dissic = collapse_depth(cube_dissic)\n",
    "\n",
    "        if mask_seaice:\n",
    "            cube_sic = mask_cube_if_no_mask(extract_region(iris.load_cube(directory2+model+'_sic_rcp85_'+ensemble+'_regridded.nc'),lon_west,lon_east,lat_south,lat_north))\n",
    "            max_seaice = cube_sic.collapsed('time',iris.analysis.MAX)\n",
    "\n",
    "            cube_tos = mask_where_seaice(cube_tos,max_seaice)\n",
    "            cube_sos = mask_where_seaice(cube_sos,max_seaice)\n",
    "            cube_fgco2 = mask_where_seaice(cube_fgco2,max_seaice) \n",
    "            cube_talk = mask_where_seaice(cube_talk,max_seaice)\n",
    "            cube_dissic = mask_where_seaice(cube_dissic,max_seaice)\n",
    "            \n",
    "        if np.max(cube_tos.data) < 200:\n",
    "            cube_tos += 273.15\n",
    "\n",
    "        print 'starting revelle calculation'\n",
    "        if np.shape(cube_talk)[0] == np.shape(cube_tos)[0] == np.shape(cube_sos)[0] == np.shape(cube_dissic)[0]: #i.e. checking all are monthly means:\n",
    "            revelle_factor = carbchem_revelle(10,9.99e9,cube_tos-273.15,cube_sos,cube_dissic,cube_talk)\n",
    "        else:\n",
    "            cube_tos = year_mean(cube_tos)\n",
    "            cube_sos = year_mean(cube_sos)\n",
    "            cube_dissic = year_mean(cube_dissic)\n",
    "            cube_talk = year_mean(cube_talk)\n",
    "            revelle_factor = carbchem_revelle(10,9.99e9,cube_tos-273.15,cube_sos,cube_dissic,cube_talk)\n",
    "        print 'ending revelle calculation'\n",
    "\n",
    "\n",
    "        cube_tos_start = avg_years(cube_tos,start_yr,end_yr)\n",
    "        cube_sos_start = avg_years(cube_sos,start_yr,end_yr)\n",
    "        cube_talk_start = avg_years(cube_talk,start_yr,end_yr)\n",
    "        cube_dissic_start = avg_years(cube_dissic,start_yr,end_yr)\n",
    "        revelle_factor_start = avg_years(revelle_factor,start_yr,end_yr)\n",
    "\n",
    "        cube_fgco2_area_avg = area_avg(cube_fgco2)\n",
    "        cube_fgco2_area_avg_start = avg_years(cube_fgco2_area_avg,start_yr,end_yr)\n",
    "        cube_fgco2_area_avg_end = avg_years(cube_fgco2_area_avg,start_yr2,end_yr2)\n",
    "        cube_fgco2_area_avg_change = cube_fgco2_area_avg_end - cube_fgco2_area_avg_start\n",
    "\n",
    "        cube_fgco2_area_sum = area_sum(cube_fgco2)\n",
    "        cube_fgco2_area_sum_start = avg_years(cube_fgco2_area_sum,start_yr,end_yr)\n",
    "        cube_fgco2_area_sum_end = avg_years(cube_fgco2_area_sum,start_yr2,end_yr2)\n",
    "        cube_fgco2_area_sum_change = cube_fgco2_area_sum_end - cube_fgco2_area_sum_start\n",
    "\n",
    "        cube_fgco2_avg_yrs = avg_years(cube_fgco2,start_yr,end_yr)\n",
    "        cube_tos_avg_yrs = avg_years(cube_tos,start_yr2,end_yr2)\n",
    "        cube_sos_avg_yrs = avg_years(cube_sos,start_yr2,end_yr2)\n",
    "        cube_dissic_avg_yrs = avg_years(cube_dissic,start_yr2,end_yr2)\n",
    "        cube_talk_avg_yrs = avg_years(cube_talk,start_yr2,end_yr2)\n",
    "        cube_fgco2_avg_yrs = avg_years(cube_fgco2,start_yr2,end_yr2)\n",
    "        revelle_factor_start_avg_yrs = area_avg(revelle_factor_start)\n",
    "\n",
    "\n",
    "        if mask_seaice:\n",
    "            with open('/disk2/ph290/pickle/'+model+'_revelle_mask_seaice_cmip5.pickle','w') as f:\n",
    "                pickle.dump([cube_tos_start,cube_sos_start,cube_dissic_start,cube_talk_start,cube_fgco2_avg_yrs,cube_tos_avg_yrs,cube_sos_avg_yrs,cube_dissic_avg_yrs,cube_talk_avg_yrs,cube_fgco2_avg_yrs,revelle_factor_start,revelle_factor_start_avg_yrs,cube_fgco2_area_avg_change,cube_fgco2_area_avg_change,cube_fgco2_area_sum_change], f)\n",
    "        else:\n",
    "            with open('/disk2/ph290/pickle/'+model+'_revelle_cmip5.pickle', 'w') as f:\n",
    "                pickle.dump([cube_tos_start,cube_sos_start,cube_dissic_start,cube_talk_start,cube_fgco2_avg_yrs,cube_tos_avg_yrs,cube_sos_avg_yrs,cube_dissic_avg_yrs,cube_talk_avg_yrs,cube_fgco2_avg_yrs,revelle_factor_start,revelle_factor_start_avg_yrs,cube_fgco2_area_avg_change,cube_fgco2_area_avg_change,cube_fgco2_area_sum_change], f)\n",
    "\n",
    "    else:\n",
    "        print 'model values already calculated'\n",
    "        if mask_seaice:\n",
    "            with open('/disk2/ph290/pickle/'+model+'_revelle_mask_seaice_cmip5.pickle','r') as f:\n",
    "                [cube_tos_start,cube_sos_start,cube_dissic_start,cube_talk_start,cube_fgco2_avg_yrs,cube_tos_avg_yrs,cube_sos_avg_yrs,cube_dissic_avg_yrs,cube_talk_avg_yrs,cube_fgco2_avg_yrs,revelle_factor_start,revelle_factor_start_avg_yrs,cube_fgco2_area_avg_change,cube_fgco2_area_avg_change,cube_fgco2_area_sum_change] = pickle.load(f)\n",
    "        else:\n",
    "            with open('/disk2/ph290/pickle/'+model+'_revelle_cmip5.pickle','r') as f:\n",
    "                [cube_tos_start,cube_sos_start,cube_dissic_start,cube_talk_start,cube_fgco2_avg_yrs,cube_tos_avg_yrs,cube_sos_avg_yrs,cube_dissic_avg_yrs,cube_talk_avg_yrs,cube_fgco2_avg_yrs,revelle_factor_start,revelle_factor_start_avg_yrs,cube_fgco2_area_avg_change,cube_fgco2_area_avg_change,cube_fgco2_area_sum_change] = pickle.load(f)\n",
    "    \n",
    "\n",
    "    if mask_seaice:\n",
    "        seaice_label = 'seaice_masked'\n",
    "    else:\n",
    "        seaice_label = 'seaice_not_masked'\n",
    "        \n",
    "    data_dict[seaice_label][model]['tos_map_start'] = cube_tos_start\n",
    "    data_dict[seaice_label][model]['sos_map_start'] = cube_sos_start\n",
    "    data_dict[seaice_label][model]['dissic_map_start'] = cube_dissic_start\n",
    "    data_dict[seaice_label][model]['talk_map_start'] = cube_talk_start\n",
    "    data_dict[seaice_label][model]['fgco2_map_start'] = cube_fgco2_avg_yrs\n",
    "    data_dict[seaice_label][model]['tos_map_end'] = cube_tos_avg_yrs\n",
    "    data_dict[seaice_label][model]['sos_map_end'] = cube_sos_avg_yrs\n",
    "    data_dict[seaice_label][model]['dissic_map_end'] = cube_dissic_avg_yrs\n",
    "    data_dict[seaice_label][model]['talk_map_end'] = cube_talk_avg_yrs\n",
    "    data_dict[seaice_label][model]['fgco2_map_end'] = cube_fgco2_avg_yrs\n",
    "    data_dict[seaice_label][model]['revelle_factor_map'] = revelle_factor_start\n",
    "    data_dict[seaice_label][model]['revelle_factor_mean'] = revelle_factor_start_avg_yrs\n",
    "    data_dict[seaice_label][model]['fgco2_change'] = cube_fgco2_area_avg_change\n",
    "    data_dict[seaice_label][model]['fgco2_change_mean'] = cube_fgco2_area_avg_change\n",
    "    data_dict[seaice_label][model]['fgco2_change_sum'] = cube_fgco2_area_sum_change\n",
    "    data_dict[seaice_label][model]['label'] = 'cmip5'\n",
    "       \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IPSL-CM5A-LR', 'MPI-ESM-LR', 'CNRM-CM5', 'CanESM2', 'IPSL-CM5B-LR', 'HadGEM2-ES', 'GFDL-ESM2G', 'HadGEM2-CC', 'CMCC-CESM', 'MPI-ESM-MR', 'GFDL-ESM2M', 'IPSL-CM5A-MR', 'CESM1-BGC']\n",
      "IPSL-CM5A-LR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "MPI-ESM-LR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "CNRM-CM5\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "CanESM2\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "IPSL-CM5B-LR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "HadGEM2-ES\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "GFDL-ESM2G\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "HadGEM2-CC\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "CMCC-CESM\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "MPI-ESM-MR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "GFDL-ESM2M\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "IPSL-CM5A-MR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "CESM1-BGC\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n"
     ]
    }
   ],
   "source": [
    "print models_cmip5\n",
    "mask_seaice = False\n",
    "if mask_seaice:\n",
    "    seaice_label = 'seaice_masked'\n",
    "else:\n",
    "    seaice_label = 'seaice_not_masked'\n",
    "data_dict[seaice_label]={}\n",
    "for model in models_cmip5:\n",
    "    print model\n",
    "    if mask_seaice:\n",
    "        data_dict['seaice_masked'][model]={}\n",
    "    else:\n",
    "        data_dict['seaice_not_masked'][model]={}\n",
    "    test1 = len(glob.glob(directory2+model+'_tos_rcp85_*_regridded.nc')) == 1\n",
    "    test2 = len(glob.glob(directory2+model+'_sos_rcp85_*_regridded.nc')) == 1\n",
    "    test3 = len(glob.glob(directory2+model+'_dissic_rcp85_*_regridded.nc')) == 1\n",
    "    test4 = len(glob.glob(directory2+model+'_talk_rcp85_*_regridded.nc')) == 1\n",
    "    if mask_seaice:\n",
    "        test5= len(glob.glob(directory2+model+'_sic_rcp85_*_regridded.nc')) == 1\n",
    "    else:\n",
    "        test5 = test4\n",
    "    print grid\n",
    "    if mask_seaice:\n",
    "        print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4,' sic:',test5\n",
    "    else:\n",
    "        print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4\n",
    "    if test1 & test2 & test3 & test4 & test5:\n",
    "        data_dict = populate_data_dict_cmip5(directory2,data_dict,model,grid,mask_seaice)\n",
    "        print 'populated data_dict with data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IPSL-CM5A-LR', 'MPI-ESM-LR', 'CNRM-CM5', 'CanESM2', 'IPSL-CM5B-LR', 'HadGEM2-ES', 'GFDL-ESM2G', 'HadGEM2-CC', 'CMCC-CESM', 'MPI-ESM-MR', 'GFDL-ESM2M', 'IPSL-CM5A-MR', 'CESM1-BGC']\n",
      "IPSL-CM5A-LR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "MPI-ESM-LR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "CNRM-CM5\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "model values already calculated\n",
      "populated data_dict with data\n",
      "CanESM2\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/CanESM2_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ph290/anaconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:798: UserWarning: Missing CF-netCDF measure variable u'areacello', referenced by netCDF variable u'talk'\n",
      "  warnings.warn(message % (variable_name, nc_var_name))\n",
      "/home/ph290/anaconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:798: UserWarning: Missing CF-netCDF measure variable u'volcello', referenced by netCDF variable u'talk'\n",
      "  warnings.warn(message % (variable_name, nc_var_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in dissic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ph290/anaconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:798: UserWarning: Missing CF-netCDF measure variable u'areacello', referenced by netCDF variable u'dissic'\n",
      "  warnings.warn(message % (variable_name, nc_var_name))\n",
      "/home/ph290/anaconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:798: UserWarning: Missing CF-netCDF measure variable u'volcello', referenced by netCDF variable u'dissic'\n",
      "  warnings.warn(message % (variable_name, nc_var_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in fgco2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ph290/anaconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:798: UserWarning: Missing CF-netCDF measure variable u'areacello', referenced by netCDF variable u'fgco2'\n",
      "  warnings.warn(message % (variable_name, nc_var_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "IPSL-CM5B-LR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/IPSL-CM5B-LR_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "HadGEM2-ES\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/HadGEM2-ES_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "GFDL-ESM2G\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/GFDL-ESM2G_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "HadGEM2-CC\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/HadGEM2-CC_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "CMCC-CESM\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/CMCC-CESM_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "MPI-ESM-MR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/MPI-ESM-MR_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "GFDL-ESM2M\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/GFDL-ESM2M_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "IPSL-CM5A-MR\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/IPSL-CM5A-MR_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n",
      "CESM1-BGC\n",
      "gn\n",
      " tos: True  sos: True  dissic: True  talk: True  sic: True\n",
      "/data/BatCaveNAS/ph290/cmip5/regridded/CESM1-BGC_dissic_rcp85_*_regridded.nc\n",
      "reading in data\n",
      "reading in tos\n",
      "reading in sos\n",
      "reading in talk\n",
      "reading in dissic\n",
      "reading in fgco2\n",
      "reading in data complete\n",
      "1\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "starting revelle calculation\n",
      "ending revelle calculation\n",
      "populated data_dict with data\n"
     ]
    }
   ],
   "source": [
    "print models_cmip5\n",
    "mask_seaice = True\n",
    "if mask_seaice:\n",
    "    seaice_label = 'seaice_masked'\n",
    "else:\n",
    "    seaice_label = 'seaice_not_masked'\n",
    "data_dict[seaice_label]={}\n",
    "for model in models_cmip5:\n",
    "    print model\n",
    "    if mask_seaice:\n",
    "        data_dict['seaice_masked'][model]={}\n",
    "    else:\n",
    "        data_dict['seaice_not_masked'][model]={}\n",
    "    test1 = len(glob.glob(directory2+model+'_tos_rcp85_*_regridded.nc')) == 1\n",
    "    test2 = len(glob.glob(directory2+model+'_sos_rcp85_*_regridded.nc')) == 1\n",
    "    test3 = len(glob.glob(directory2+model+'_dissic_rcp85_*_regridded.nc')) == 1\n",
    "    test4 = len(glob.glob(directory2+model+'_talk_rcp85_*_regridded.nc')) == 1\n",
    "    if mask_seaice:\n",
    "        test5= len(glob.glob(directory2+model+'_sic_rcp85_*_regridded.nc')) == 1\n",
    "    else:\n",
    "        test5 = test4\n",
    "    print grid\n",
    "    if mask_seaice:\n",
    "        print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4,' sic:',test5\n",
    "    else:\n",
    "        print ' tos:',test1,' sos:',test2,' dissic:',test3,' talk:',test4\n",
    "    if test1 & test2 & test3 & test4 & test5:\n",
    "        data_dict = populate_data_dict_cmip5(directory2,data_dict,model,grid,mask_seaice)\n",
    "        print 'populated data_dict with data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNRM-ESM2-1', 'GFDL-CM4', 'MPI-ESM1-2-HR', 'CESM2', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC-ES2L']\n",
      "CNRM-ESM2-1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CNRM-ESM2-1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d4a03c26ca8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     print data_dict[model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     y = data_dict[model]['fgco2_change_sum'].data*yearsec*1.0e-12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fgco2_change_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0myearsec\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkg2mol_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'revelle_factor_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CNRM-ESM2-1'"
     ]
    }
   ],
   "source": [
    "mask_seaice = True\n",
    "if mask_seaice:\n",
    "    seaice_label = 'seaice_masked'\n",
    "else:\n",
    "    seaice_label = 'seaice_not_masked'\n",
    "# print models_cimp5\n",
    "# models = list(models_cmip6)\n",
    "models=list(models_cmip6)\n",
    "# models.remove('GFDL-CM4')\n",
    "print models\n",
    "for model in models:\n",
    "    print model\n",
    "#     print data_dict[model]\n",
    "#     y = data_dict[model]['fgco2_change_sum'].data*yearsec*1.0e-12\n",
    "    y = data_dict[seaice_label][model]['fgco2_change_mean'].data*yearsec*kg2mol_C\n",
    "    x = data_dict[seaice_label][model]['revelle_factor_mean'].data\n",
    "    label = data_dict[seaice_label][model]['label']\n",
    "    if label == 'cmip5':\n",
    "        my_color='r'\n",
    "        my_marker = '*'\n",
    "    if label == 'cmip6':\n",
    "        my_color='b'\n",
    "        my_marker = 'o'\n",
    "    if model == 'MIROC-ES2L':\n",
    "        print 'assuming '+model+' has submitted in units of CO2 rather than C'\n",
    "        y *= (12.0/44.01)\n",
    "    if model == 'CanESM2':\n",
    "        print 'assuming '+model+' has submitted in units of CO2 rather than C'\n",
    "        y *= (12.0/44.01)\n",
    "    plt.scatter(x,y,label=model,s=100,marker=my_marker)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.4, 1.00))\n",
    "plt.xlabel('Revelle Factor')\n",
    "# plt.ylabel('air-sea CO$_2$ flux change\\n(mol m$^{-1}$ yr$^{-1}$)')\n",
    "plt.ylabel('air-sea CO$_2$ flux change\\n(PgCyr$^{-1}$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['dissic','talk','tos','sos','fgco2','areacello']\n",
    "\n",
    "\n",
    "models_cimp5 = list(model_names_var_cmip5(directory2,variables[0]))\n",
    "\n",
    "#check to see if all variables available\n",
    "for var in variables[1::]:\n",
    "    models1 = list(model_names_var_cmip5(directory2,var))\n",
    "    models_cimp5 = list(set(models_cimp5).intersection(models1))\n",
    "\n",
    "print models_cimp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_data_dict_cmip5(directory1,data_dict,model):\n",
    "    run = 'rcp85'\n",
    "#     print directory1+model+'_dissic_+'_Socean.nc'  HadGEM2-CC_dissic_rcp85_r1i1p1_Socean.nc\n",
    "    file = glob.glob(directory1+model+'_dissic_'+run+'_*_Socean.nc')[0]\n",
    "    ensemble = file.split('_')[3]\n",
    "#     start_yr,end_yr = 2015,2035\n",
    "    start_yr,end_yr = 2005,2025\n",
    "\n",
    "\n",
    "    start_yr2,end_yr2 = 2079,2099\n",
    "\n",
    "    cube_tos = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_tos_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_sos = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_sos_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_talk = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_talk_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_dissic = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_dissic_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_fgco2 = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_fgco2_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_areacello = iris.load_cube(directory1+model+'_areacello_cmip5_Socean.nc')\n",
    "    if model == 'MPI-ESM-LR':\n",
    "        cube_areacello.data = np.flipud(cube_areacello.data)\n",
    "#     try:\n",
    "#         cube_areacello = cube_areacello.collapsed('time',iris.analysis.MEAN)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "    cube_tos_start = avg_years(cube_tos,start_yr,end_yr)\n",
    "    cube_sos_start = avg_years(cube_sos,start_yr,end_yr)\n",
    "    cube_talk_start = avg_years(cube_talk,start_yr,end_yr)\n",
    "    cube_dissic_start = avg_years(cube_dissic,start_yr,end_yr)\n",
    "\n",
    "    if np.max(cube_tos_start.data) < 200:\n",
    "        cube_tos_start += 273.15\n",
    "\n",
    "    revelle_factor = carbchem_revelle(10,9.99e9,cube_tos_start-273.15,cube_sos_start,cube_dissic_start,cube_talk_start)\n",
    "#     revelle_factor = revelle_factor.collapsed('time',iris.analysis.MEAN)\n",
    "\n",
    "    cube_fgco2_area_avg = area_avg_with_areacello(cube_fgco2,cube_areacello)\n",
    "    cube_fgco2_area_avg_start = avg_years(cube_fgco2_area_avg,start_yr,end_yr)\n",
    "    cube_fgco2_area_avg_end = avg_years(cube_fgco2_area_avg,start_yr2,end_yr2)\n",
    "    cube_fgco2_area_avg_change = cube_fgco2_area_avg_end - cube_fgco2_area_avg_start\n",
    "    \n",
    "    cube_fgco2_area_sum = area_sum_with_areacello(cube_fgco2,cube_areacello)\n",
    "    cube_fgco2_area_sum_start = avg_years(cube_fgco2_area_sum,start_yr,end_yr)\n",
    "    cube_fgco2_area_sum_end = avg_years(cube_fgco2_area_sum,start_yr2,end_yr2)\n",
    "    cube_fgco2_area_sum_change = cube_fgco2_area_sum_end - cube_fgco2_area_sum_start\n",
    "    \n",
    "\n",
    "#     cube_fgco2_global = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_fgco2_'+run+'_'+ensemble+'.nc','surface_downward_mass_flux_of_carbon_dioxide_expressed_as_carbon')))\n",
    "#     cube_areacello_global = iris.load_cube(directory1+model+'_areacello_cmip5.nc')\n",
    "#     test = cube_fgco2_global.copy()\n",
    "#     test *= 0.0\n",
    "#     test += 1.0\n",
    "#     ocean_area = area_sum_with_areacello(test,cube_areacello_global)\n",
    "    \n",
    "#     tmp = revelle_factor.copy()\n",
    "#     tmp *= 0.0\n",
    "#     tmp += 1.0\n",
    "#     area_avg_revelle = area_sum_with_areacello(revelle_factor,cube_areacello) / area_sum_with_areacello(tmp,cube_areacello)\n",
    "    area_avg_revelle = area_avg_with_areacello(revelle_factor,cube_areacello)\n",
    "    \n",
    "    data_dict[model]['tos_map_start'] = cube_tos_start\n",
    "    data_dict[model]['sos_map_start'] = cube_sos_start\n",
    "    data_dict[model]['dissic_map_start'] = cube_dissic_start\n",
    "    data_dict[model]['talk_map_start'] = cube_talk_start\n",
    "    data_dict[model]['fgco2_map_start'] = avg_years(cube_fgco2,start_yr,end_yr)\n",
    "    data_dict[model]['tos_map_end'] = avg_years(cube_tos,start_yr2,end_yr2)\n",
    "    data_dict[model]['sos_map_end'] = avg_years(cube_sos,start_yr2,end_yr2)\n",
    "    data_dict[model]['dissic_map_end'] = avg_years(cube_dissic,start_yr2,end_yr2)\n",
    "    data_dict[model]['talk_map_end'] = avg_years(cube_talk,start_yr2,end_yr2)\n",
    "    data_dict[model]['fgco2_map_end'] = avg_years(cube_fgco2,start_yr2,end_yr2)\n",
    "    data_dict[model]['revelle_factor_map'] = revelle_factor\n",
    "    data_dict[model]['revelle_factor_mean'] = area_avg_revelle\n",
    "    data_dict[model]['fgco2_change_mean'] = cube_fgco2_area_avg_change\n",
    "    data_dict[model]['fgco2_change_sum'] = cube_fgco2_area_sum_change\n",
    "    data_dict[model]['areacello'] = cube_areacello\n",
    "#     data_dict[model]['fgco2'] = cube_fgco2\n",
    "    data_dict[model]['label'] = 'cmip5'\n",
    "#     data_dict[model]['ocean_area'] = ocean_area\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_cimp5[0]\n",
    "print directory2+model+'_fgco2_'+'rcp85'+'_'+'r1i1p1'+'_Socean.nc'\n",
    "iris.load_cube(directory2+model+'_fgco2_'+'rcp85'+'_'+'r1i1p1'+'_Socean.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_cimp5:\n",
    "    print model\n",
    "    data_dict[model]={}\n",
    "    data_dict = populate_data_dict_cmip5(directory2,data_dict,model)\n",
    "# except:\n",
    "#     grid = 'gr'\n",
    "#     data_dict = populate_data_dict(directory1,data_dict,model,grid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models_cimp5:\n",
    "#     print model\n",
    "#     print data_dict[model]['ocean_area'][0].data*1.0e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_cimp5:\n",
    "    print model\n",
    "    qplt.contourf(data_dict[model]['areacello'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'MPI-ESM-LR'\n",
    "run = 'rcp85'\n",
    "ensemble = 'r1i1p1'\n",
    "# cube_fgco2 = mask_cube_if_no_mask(year_mean(iris.load_cube(directory2+model+'_fgco2_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "cube_fgco2 = iris.load_cube(directory2+model+'_fgco2_'+run+'_'+ensemble+'_Socean.nc')\n",
    "print np.shape(cube_fgco2)\n",
    "# print cube_fgco2\n",
    "# print cube_fgco2.coord('latitude')\n",
    "# print cube_fgco2.data[0,:]\n",
    "\n",
    "\n",
    "# cube_fgco2.coord('longutude').points\n",
    "# print data_dict[model]['areacello']\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "gs = gridspec.GridSpec(3,1)\n",
    "ax1 = plt.subplot(gs[0,0],projection=ccrs.PlateCarree())\n",
    "#     plt.contourf(data_dict[model]['revelle_factor_map'].data,21)\n",
    "plt.pcolormesh(cube_fgco2[0].data)\n",
    "ax1 = plt.subplot(gs[1,0])\n",
    "plt.pcolormesh(data_dict[model]['areacello'].data)\n",
    "ax1 = plt.subplot(gs[2,0])\n",
    "\n",
    "plt.pcolormesh(data_dict[model]['areacello'].data * cube_fgco2[0].data)\n",
    "\n",
    "\n",
    "plt.colorbar()\n",
    "plt.title(model)\n",
    "#     plt.gca().coastlines()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = iris.load_cube('/Users/ph290/Downloads/revelle/cmip5/MPI-ESM-LR_areacello_cmip5.nc')\n",
    "tmp2 = iris.load_cube('/Users/ph290/Downloads/revelle/cmip5/MPI-ESM-LR_fgco2_rcp85_r1i1p1.nc')[0]\n",
    "# print tmp \n",
    "qplt.contourf(tmp,100)\n",
    "plt.show()\n",
    "qplt.contourf(tmp2)\n",
    "\n",
    "\n",
    "# plt.gca().coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = models_cmip6+models_cimp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "gs = gridspec.GridSpec(len(models),1)\n",
    "for i,model in enumerate(models):\n",
    "    ax1 = plt.subplot(gs[i:i+1,0],projection=ccrs.PlateCarree())\n",
    "#     plt.contourf(data_dict[model]['revelle_factor_map'].data,21)\n",
    "    plt.contourf(data_dict[model]['fgco2_map_start'].data,21)\n",
    "    plt.colorbar()\n",
    "    plt.title(model)\n",
    "#     plt.gca().coastlines()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model= 'IPSL-CM6A-LR'\n",
    "# #     plt.contourf(data_dict[model]['revelle_factor_map'].data,21)\n",
    "# plt.contourf(data_dict[model]['tos_map_start'].data)\n",
    "# plt.colorbar()\n",
    "# plt.title(model)\n",
    "# #     plt.gca().coastlines()\n",
    "    \n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# model= 'GFDL-CM4'\n",
    "# plt.contourf(data_dict[model]['tos_map_start'].data)\n",
    "# plt.colorbar()\n",
    "# plt.title(model)\n",
    "# #     plt.gca().coastlines()\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHY DO THE CMIP5 RUNS BELOW NOT LOOK LIKE PREVIOUSLY WHERE I REGRIDDED AVERYTHING????\n",
    "e.g. why is teh flux different in MPI-ESM-LR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi model mean change in update last 20 years - first 20 yeats \n",
    "greyed is where fewer than 66% of models agree on the sign of change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revelle each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtropical relationship between revelle and change in CO2 flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not Just N. Atl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global air-sea flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subpolar relationship between revelle and change in CO2 flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glodap_dic_file ='/data/NAS-ph290/ph290/observations/GLODAPv2_Mapped_Climatologies/GLODAPv2.2016b_MappedClimatologies/GLODAPv2.2016b.TCO2.nc'\n",
    "# glodap_talk_file ='/data/NAS-ph290/ph290/observations/GLODAPv2_Mapped_Climatologies/GLODAPv2.2016b_MappedClimatologies/GLODAPv2.2016b.TAlk.nc'\n",
    "# glodap_dic = iris.load_cube(glodap_dic_file,'moles of dissolved inorganic carbon per unit mass in seawater')[0]/1026.0\n",
    "# glodap_talk = iris.load_cube(glodap_talk_file,'seawater alkalinity expressed as mole equivalent per unit mass')[0]/1026.0\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=-80.0,10,0.0,80.0\n",
    "# glodap_dic_n_atl = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_n_atl = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=-80.0,10,-45,0\n",
    "# glodap_dic_s_atl = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_s_atl = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=-180.0,180.0,-90.0,-45.0\n",
    "# glodap_dic_so = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_so = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=130,360-100,0,60\n",
    "# glodap_dic_np = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_np = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=130,360-100,-45,0\n",
    "# glodap_dic_sp = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_sp = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=42,100,-45,20\n",
    "# glodap_dic_io = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_io = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
